# ============================================================================
# REFINED uncertainty_analysis_refined.py
# Stage 4: Enhanced Analysis with Backtesting & Multi-Confidence Levels
# ============================================================================

import numpy as np
import torch
from scipy.stats import norm
from typing import Dict, Tuple
import warnings
warnings.filterwarnings('ignore')


class UncertaintyEstimator:
    """ë¶ˆí™•ì‹¤ì„± ì¶”ì • (ê°œì„ : multi-confidence level ì§€ì›)"""
    
    def __init__(self, model, device: str = 'cpu'):
        self.model = model
        self.device = device
        self.model.eval()
    
    def estimate_uncertainties(self, X_test: np.ndarray, 
                              n_mc_samples: int = 100) -> Dict[str, np.ndarray]:
        """MC Dropout ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ì¶”ì •"""
        X_test_tensor = torch.FloatTensor(X_test).to(self.device)
        
        print("Running MC Dropout inference...")
        mc_predictions = self.model.mc_dropout_forward(X_test_tensor, n_samples=n_mc_samples)
        mc_predictions = mc_predictions.cpu().numpy()
        
        epistemic_std = mc_predictions.std(axis=1)
        mc_mean = mc_predictions.mean(axis=1)
        
        self.model.eval()
        with torch.no_grad():
            _, aleatoric_std_pred, _ = self.model(X_test_tensor)
        
        aleatoric_std = aleatoric_std_pred.squeeze().cpu().numpy()
        total_std = np.sqrt(epistemic_std**2 + aleatoric_std**2)
        
        print(f"âœ“ Epistemic uncertainty: {epistemic_std.mean():.6f} Â± {epistemic_std.std():.6f}")
        print(f"âœ“ Aleatoric uncertainty: {aleatoric_std.mean():.6f} Â± {aleatoric_std.std():.6f}")
        print(f"âœ“ Total uncertainty: {total_std.mean():.6f} Â± {total_std.std():.6f}")
        
        return {
            'predictions': mc_mean,
            'epistemic_std': epistemic_std,
            'aleatoric_std': aleatoric_std,
            'total_std': total_std,
            'mc_predictions': mc_predictions
        }


class CalibrationEvaluator:
    """Calibration í‰ê°€ (ê°œì„ : Multi-confidence level)"""
    
    @staticmethod
    def compute_calibration_metrics(predictions: np.ndarray, 
                                   uncertainties: np.ndarray,
                                   targets: np.ndarray,
                                   confidence_levels: list = None) -> Dict:
        """
        ê°œì„ : Multiple confidence levels ë™ì‹œ ì§€ì›
        ì´ìƒì : coverage â‰ˆ confidence_level (ì˜¤ì°¨ < 2%)
        """
        if confidence_levels is None:
            confidence_levels = [0.68, 0.95, 0.99]
        
        metrics = {}
        
        for confidence in confidence_levels:
            z_score = norm.ppf((1 + confidence) / 2)
            
            lower = predictions - z_score * uncertainties
            upper = predictions + z_score * uncertainties
            
            coverage = np.mean((targets >= lower) & (targets <= upper))
            interval_width = np.mean(upper - lower)
            calibration_error = np.abs(coverage - confidence)
            
            # Average interval score
            ais = interval_width + (2/z_score) * np.maximum(lower - targets, 0) + \
                  (2/z_score) * np.maximum(targets - upper, 0)
            ais = ais.mean()
            
            metrics[f'{int(confidence*100)}%'] = {
                'coverage': coverage,
                'target': confidence,
                'error': calibration_error,
                'interval_width': interval_width,
                'average_interval_score': ais
            }
        
        return metrics
    
    @staticmethod
    def print_calibration_analysis(metrics: Dict) -> None:
        """Calibration ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*90)
        print("CALIBRATION ANALYSIS (ê°œì„ : ì—¬ëŸ¬ ì‹ ë¢°ë„ ìˆ˜ì¤€ ê²€ì¦)")
        print("="*90)
        
        print(f"\n{'Confidence':<12} {'Target':<10} {'Achieved':<10} {'Error':<10} {'Status':<10}")
        print("-" * 90)
        
        for conf_level, metric_dict in metrics.items():
            coverage = metric_dict['coverage']
            target = metric_dict['target']
            error = metric_dict['error']
            
            # Status check
            if error < 0.02:
                status = "âœ“ Excellent"
            elif error < 0.03:
                status = "âœ“ Good"
            else:
                status = "âœ— Poor"
            
            print(f"{conf_level:<12} {target:>9.0%}  {coverage:>9.0%}  {error:>9.4f}  {status:<10}")
        
        print("\nâœ“ Calibration ê¸°ì¤€ (ì˜¤ì°¨ < 2%): Modelì´ ì‹ ë¢°ë„ êµ¬ê°„ì„ ì •í™•íˆ ì œì‹œ")


class RegulatoryBacktesting:
    """
    ê°œì„ : Regulatory Backtesting ì¶”ê°€
    Basel IIIì˜ Backtesting í”„ë ˆì„ì›Œí¬ ì ìš©
    """
    
    @staticmethod
    def kupiec_pof_test(predictions: np.ndarray, targets: np.ndarray,
                       confidence: float = 0.95) -> Dict:
        """
        Kupiec's Proportion of Failures (POF) Test
        
        Null hypothesis: ì‹¤íŒ¨ìœ¨ = (1 - confidence)
        â†’ H0ë¥¼ ê¸°ê°í•˜ì§€ ëª»í•˜ë©´ ëª¨ë¸ì´ good calibration
        """
        n = len(targets)
        failures = np.sum(targets < predictions)
        failure_rate = failures / n
        expected_failure_rate = 1 - confidence
        
        # POF statistic
        if failure_rate > 0 and failure_rate < 1:
            lr_pof = 2 * (failures * np.log(failure_rate / expected_failure_rate) +
                         (n - failures) * np.log((1 - failure_rate) / (1 - expected_failure_rate)))
        else:
            lr_pof = 0
        
        # Critical value (chi-squared with df=1, alpha=0.05)
        critical_value = 3.841
        pof_pass = lr_pof < critical_value
        
        return {
            'failures': failures,
            'failure_rate': failure_rate,
            'expected_rate': expected_failure_rate,
            'lr_statistic': lr_pof,
            'critical_value': critical_value,
            'passes': pof_pass
        }
    
    @staticmethod
    def traffic_light_approach(predictions: np.ndarray, targets: np.ndarray,
                             confidence: float = 0.95, window: int = 252) -> Dict:
        """
        Basel III Traffic Light Approach
        
        Green: 4ê°œ ì´í•˜ exceptions â†’ No action
        Yellow: 5-9ê°œ exceptions â†’ Further analysis
        Red: 10ê°œ ì´ìƒ exceptions â†’ Model rejected
        """
        exceptions = np.sum(targets < predictions)
        
        if exceptions <= 4:
            zone = "ğŸŸ¢ Green Zone"
            action = "No regulatory action needed"
        elif exceptions <= 9:
            zone = "ğŸŸ¡ Yellow Zone"
            action = "Further investigation required"
        else:
            zone = "ğŸ”´ Red Zone"
            action = "Model must be rejected/revised"
        
        return {
            'exceptions': exceptions,
            'zone': zone,
            'action': action
        }
    
    @staticmethod
    def print_backtesting_results(pof_results: Dict, tl_results: Dict) -> None:
        """Backtesting ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*90)
        print("REGULATORY BACKTESTING (ê°œì„ : Basel III í”„ë ˆì„ì›Œí¬ ì ìš©)")
        print("="*90)
        
        print("\nã€Kupiec POF Testã€‘")
        print(f"  Failures: {pof_results['failures']}")
        print(f"  Failure Rate: {pof_results['failure_rate']:.2%} (Expected: {pof_results['expected_rate']:.2%})")
        print(f"  LR Statistic: {pof_results['lr_statistic']:.4f} (Critical: {pof_results['critical_value']:.4f})")
        print(f"  Result: {'âœ“ PASS' if pof_results['passes'] else 'âœ— FAIL'}")
        
        print("\nã€Traffic Light Approachã€‘")
        print(f"  Zone: {tl_results['zone']}")
        print(f"  Action: {tl_results['action']}")


class SensitivityAnalysis:
    """
    ê°œì„ : Sensitivity Analysis ì¶”ê°€
    ëª¨ë¸ì˜ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì˜í–¥ë„ ë¶„ì„
    """
    
    @staticmethod
    def dropout_rate_sensitivity(model, X_test: np.ndarray, y_test: np.ndarray,
                                dropout_rates: list = [0.1, 0.2, 0.3]) -> Dict:
        """
        Dropout rate ë³€í™”ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
        (í˜„ì¬ëŠ” ê³ ì •ëœ ëª¨ë¸ì´ë¯€ë¡œ ê°œë… ì„¤ëª…ë§Œ)
        """
        results = {}
        
        print("\n" + "="*70)
        print("SENSITIVITY ANALYSIS: Dropout Rate Impact")
        print("="*70)
        print("\nâš ï¸ Note: This shows impact of different dropout rates")
        print("         (Implementation requires model retraining)")
        print(f"\nDropout rates tested: {dropout_rates}")
        print("Expected impact: Higher dropout â†’ Larger epistemic uncertainty")
        
        return results
    
    @staticmethod
    def mc_samples_sensitivity(model, X_test: np.ndarray, 
                             mc_samples_list: list = [10, 50, 100, 200]) -> Dict:
        """
        MC sample ìˆ˜ì— ë”°ë¥¸ epistemic uncertainty ìˆ˜ë ´
        """
        print("\n" + "="*70)
        print("SENSITIVITY ANALYSIS: MC Samples Impact")
        print("="*70)
        
        results = {}
        
        for n_samples in mc_samples_list:
            X_tensor = torch.FloatTensor(X_test).to('cpu')
            
            model.eval()
            model.train()  # MC Dropout í™œì„±í™”
            
            mc_preds = model.mc_dropout_forward(X_tensor, n_samples=n_samples)
            epistemic_std = mc_preds.std(axis=1).cpu().numpy()
            
            results[n_samples] = {
                'mean_epistemic': epistemic_std.mean(),
                'std_epistemic': epistemic_std.std()
            }
            
            print(f"{n_samples} samples: "
                  f"Epistemic = {epistemic_std.mean():.6f} "
                  f"(converges as n â†’ âˆ)")
        
        return results


def comprehensive_analysis(model, X_test: np.ndarray, y_test: np.ndarray,
                          device: str = 'cpu') -> Dict:
    """
    ê°œì„ : ì¢…í•© ë¶„ì„ (Calibration + Backtesting + Sensitivity)
    """
    print("\n" + "="*90)
    print("COMPREHENSIVE UNCERTAINTY ANALYSIS (IMPROVED)")
    print("="*90)
    
    # 1. Uncertainty Estimation
    estimator = UncertaintyEstimator(model, device)
    uncertainty_results = estimator.estimate_uncertainties(X_test, n_mc_samples=100)
    
    # 2. Calibration Analysis (Multi-confidence)
    calibration_metrics = CalibrationEvaluator.compute_calibration_metrics(
        uncertainty_results['predictions'],
        uncertainty_results['total_std'],
        y_test,
        confidence_levels=[0.68, 0.95, 0.99]  # ê°œì„ : ì—¬ëŸ¬ ì‹ ë¢°ë„
    )
    CalibrationEvaluator.print_calibration_analysis(calibration_metrics)
    
    # 3. Regulatory Backtesting (NEW)
    pof_results = RegulatoryBacktesting.kupiec_pof_test(
        uncertainty_results['predictions'], y_test, confidence=0.95
    )
    tl_results = RegulatoryBacktesting.traffic_light_approach(
        uncertainty_results['predictions'], y_test, confidence=0.95
    )
    RegulatoryBacktesting.print_backtesting_results(pof_results, tl_results)
    
    # 4. Sensitivity Analysis (NEW)
    mc_sensitivity = SensitivityAnalysis.mc_samples_sensitivity(
        model, X_test, mc_samples_list=[10, 50, 100, 200]
    )
    
    return {
        'uncertainties': uncertainty_results,
        'calibration': calibration_metrics,
        'backtesting_pof': pof_results,
        'backtesting_tl': tl_results,
        'sensitivity': mc_sensitivity
    }


def main():
    """Main execution"""
    print("Loading trained model...")
    from model_refined import BayesianVaRNN
    
    model = BayesianVaRNN(input_dim=11, hidden_dim=128, dropout_rate=0.2)
    model.load_state_dict(torch.load('best_bayesian_var_model.pt'))
    
    print("Loading test data...")
    data = np.load('./data/synthetic_data.npz')
    X_val = data['X_val']
    y_val = data['y_val']
    
    # Comprehensive analysis
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    results = comprehensive_analysis(model, X_val, y_val, device)
    
    return results


if __name__ == '__main__':
    results = main()
