import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import json
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class SP500MetaLearningPreprocessor:
    """
    S&P 500 ë°ì´í„° ì „ì²˜ë¦¬ ë° Meta-Learning Episode ìƒì„±
    
    ë…¼ë¬¸ì˜ ì‹¤í—˜ ì„¤ê³„ì— ë§ì¶˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
    - 39 features + 1 target (returns)
    - K=4 market regimes (k-means clustering)
    - Regime-pure episodes (support/query ë™ì¼ regime)
    - Time-ordered train/val/test split
    - N-asset subset experiments (N=10,50,100,200,400,all)
    """
    
    def __init__(self, data_dir='./sp500_data', output_dir='./processed_data'):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.panel_data = None
        self.regimes = None
        self.episodes = None
        
        os.makedirs(output_dir, exist_ok=True)
        print(f"Preprocessor ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - Input: {data_dir}")
        print(f"  - Output: {output_dir}")
    
    def load_panel_data(self):
        """Panel ë°ì´í„° ë¡œë“œ"""
        print("\n" + "="*80)
        print("Panel ë°ì´í„° ë¡œë“œ ì‹œì‘")
        print("="*80)
        
        panel_path = os.path.join(self.data_dir, 'sp500_panel.csv')
        
        # CSV ë¡œë“œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ ì¸ë±ìŠ¤)
        self.panel_data = pd.read_csv(panel_path, index_col=0, parse_dates=True)
        
        print(f"  ë¡œë“œëœ shape: {self.panel_data.shape}")
        print(f"  Index name: {self.panel_data.index.name}")
        print(f"  Columns: {list(self.panel_data.columns[:5])}...")
        
        # Row index ì¶”ê°€ (PyTorch Datasetìš©)
        # reset_index()ë¡œ ë‚ ì§œë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
        self.panel_data = self.panel_data.reset_index()
        
        # ì¸ë±ìŠ¤ ì»¬ëŸ¼ëª… í™•ì¸ ë° í†µì¼
        date_col = self.panel_data.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ
        print(f"  Date column detected: '{date_col}'")
        
        # ë‚ ì§œ ì»¬ëŸ¼ëª…ì„ 'date'ë¡œ í†µì¼
        if date_col != 'date':
            self.panel_data = self.panel_data.rename(columns={date_col: 'date'})
            print(f"  Renamed '{date_col}' â†’ 'date'")
        
        # ë‚ ì§œ íƒ€ì… í™•ì¸
        self.panel_data['date'] = pd.to_datetime(self.panel_data['date'])
        
        # Row ID ì¶”ê°€
        self.panel_data['row_id'] = np.arange(len(self.panel_data))
        
        # ë‚ ì§œë¥¼ ë‹¤ì‹œ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
        self.panel_data = self.panel_data.set_index('date')
        
        print(f"\nâœ“ Panel ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"  - Shape: {self.panel_data.shape}")
        print(f"  - Index: {self.panel_data.index.name} (dtype: {self.panel_data.index.dtype})")
        print(f"  - Columns: {len(self.panel_data.columns)}")
        print(f"  - Features: 39 (+ returns + ticker + row_id)")
        print(f"  - Memory: {self.panel_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        return self.panel_data

    
    def eda_summary(self):
        """íŒ¨ë„ ë°ì´í„° EDA (ê°„ëµ)"""
        print("\n" + "="*80)
        print("íŒ¨ë„ ë°ì´í„° EDA")
        print("="*80)
        
        df = self.panel_data
        
        # 1. ê¸°ë³¸ ì •ë³´
        print("\n[1] ê¸°ë³¸ ì •ë³´")
        print(f"  - ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(df):,}")
        print(f"  - ìœ ë‹ˆí¬ í‹°ì»¤ ìˆ˜: {df['ticker'].nunique()}")
        print(f"  - ë‚ ì§œ ë²”ìœ„: {df.index.min()} ~ {df.index.max()}")
        print(f"  - ì´ ê¸°ê°„: {(df.index.max() - df.index.min()).days} days (~{(df.index.max() - df.index.min()).days/252:.1f} years)")
        
        # 2. Missing ë¹„ìœ¨
        print("\n[2] Missing Values")
        feature_cols = [col for col in df.columns if col not in ['ticker', 'returns', 'row_id']]
        missing_ratio = (df[feature_cols].isnull().sum() / len(df) * 100).sort_values(ascending=False)
        high_missing = missing_ratio[missing_ratio > 5]
        
        if len(high_missing) > 0:
            print(f"  âš  Missing > 5%ì¸ ì»¬ëŸ¼ ({len(high_missing)}ê°œ):")
            for col, ratio in high_missing.head(10).items():
                print(f"    - {col}: {ratio:.2f}%")
        else:
            print(f"  âœ“ ëª¨ë“  featureì˜ missing < 5%")
        
        print(f"  - ì „ì²´ í‰ê·  missing: {missing_ratio.mean():.3f}%")
        
        # 3. Extreme ê°’ ì²´í¬
        print("\n[3] Extreme Values ì²´í¬ (Â±5 std)")
        extreme_counts = {}
        for col in feature_cols:
            mean = df[col].mean()
            std = df[col].std()
            if std > 0:
                extreme = ((df[col] < mean - 5*std) | (df[col] > mean + 5*std)).sum()
                if extreme > 0:
                    extreme_counts[col] = extreme
        
        if extreme_counts:
            total_extremes = sum(extreme_counts.values())
            print(f"  âš  Extreme ê°’ ì´ {total_extremes:,}ê°œ ({total_extremes/len(df)/len(feature_cols)*100:.3f}%)")
            print(f"    ì˜í–¥ë°›ëŠ” ì»¬ëŸ¼: {len(extreme_counts)}ê°œ")
            top_extreme = sorted(extreme_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            for col, count in top_extreme:
                print(f"      - {col}: {count:,}")
        else:
            print(f"  âœ“ Extreme ê°’ ì—†ìŒ")
        
        # 4. Returns ë¶„í¬
        print("\n[4] Returns ë¶„í¬")
        returns = df['returns'].dropna()
        print(f"  - Mean: {returns.mean():.6f} ({returns.mean()*252:.2%} annualized)")
        print(f"  - Std: {returns.std():.6f} ({returns.std()*np.sqrt(252):.2%} annualized)")
        print(f"  - Skewness: {returns.skew():.3f}")
        print(f"  - Kurtosis: {returns.kurtosis():.3f}")
        print(f"  - Min/Max: [{returns.min():.3f}, {returns.max():.3f}]")
        
        # 5. í‹°ì»¤ë³„ ë¶„í¬
        print("\n[5] í‹°ì»¤ë³„ ë°ì´í„° ë¶„í¬")
        ticker_counts = df.groupby('ticker').size()
        print(f"  - í‰ê·  ê´€ì¸¡ì¹˜/í‹°ì»¤: {ticker_counts.mean():.0f}")
        print(f"  - ì¤‘ì•™ê°’: {ticker_counts.median():.0f}")
        print(f"  - ìµœì†Œ/ìµœëŒ€: [{ticker_counts.min()}, {ticker_counts.max()}]")
        
        return {
            'total_points': len(df),
            'n_tickers': df['ticker'].nunique(),
            'date_range': (df.index.min(), df.index.max()),
            'missing_ratio': missing_ratio.to_dict(),
            'extreme_counts': extreme_counts
        }
    
    def compute_market_summary(self):
        """Daily market summary ê³„ì‚° (regime ë¼ë²¨ë§ìš©)"""
        print("\n" + "="*80)
        print("Market Summary ê³„ì‚° (Regime Clustering)")
        print("="*80)
        
        df = self.panel_data
        
        # ë‚ ì§œë³„ë¡œ ì§‘ê³„
        print("\nì¼ë³„ ì‹œì¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        daily_summary = df.groupby(df.index).agg({
            'returns': ['mean', 'std'],
            'volume': 'sum',
            'vix': 'first',
            'treasury_10y': 'first',
            'treasury_2y': 'first',
            'yield_spread': 'first',
            'usd_index': 'first',
            'close': 'count'
        }).copy()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        daily_summary.columns = ['_'.join(col).strip() for col in daily_summary.columns.values]
        daily_summary.rename(columns={
            'returns_mean': 'market_return',
            'returns_std': 'market_volatility',
            'volume_sum': 'total_volume',
            'vix_first': 'vix',
            'treasury_10y_first': 'treasury_10y',
            'treasury_2y_first': 'treasury_2y',
            'yield_spread_first': 'yield_spread',
            'usd_index_first': 'usd_index',
            'close_count': 'n_assets'
        }, inplace=True)
        
        # ì¶”ê°€ ì§€í‘œ
        daily_summary['volume_change'] = daily_summary['total_volume'].pct_change()
        daily_summary['vix_change'] = daily_summary['vix'].pct_change()
        
        # ì´ë™í‰ê·  (regime íŠ¹ì§•í™”)
        for window in [5, 20]:
            daily_summary[f'ma_return_{window}d'] = daily_summary['market_return'].rolling(window).mean()
            daily_summary[f'ma_vol_{window}d'] = daily_summary['market_volatility'].rolling(window).mean()
        
        daily_summary = daily_summary.dropna()
        
        print(f"âœ“ Daily summary ê³„ì‚° ì™„ë£Œ: {daily_summary.shape}")
        
        return daily_summary
    
    def label_regimes(self, n_regimes=4, features=None, random_state=42):
        """
        K-meansë¥¼ ì‚¬ìš©í•œ Market Regime ë¼ë²¨ë§
        
        Args:
            n_regimes: Regime ê°œìˆ˜ (K). ë…¼ë¬¸ì—ì„œëŠ” K=4 ì‚¬ìš©
            features: Clusteringì— ì‚¬ìš©í•  feature ë¦¬ìŠ¤íŠ¸
            random_state: ì¬í˜„ì„±ì„ ìœ„í•œ seed
        """
        print("\n" + "="*80)
        print(f"Market Regime ë¼ë²¨ë§ (K={n_regimes})")
        print("="*80)
        
        # Market summary ê³„ì‚°
        daily_summary = self.compute_market_summary()
        
        # Clustering features
        if features is None:
            features = [
                'market_return', 'market_volatility', 'vix',
                'treasury_10y', 'yield_spread',
                'ma_return_5d', 'ma_vol_5d',
                'ma_return_20d', 'ma_vol_20d'
            ]
        
        print(f"\nClustering features ({len(features)}ê°œ):")
        for f in features:
            print(f"  - {f}")
        
        X = daily_summary[features].values
        
        # í‘œì¤€í™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # K-means
        print(f"\nK-means í•™ìŠµ ì¤‘ (K={n_regimes}, seed={random_state})...")
        kmeans = KMeans(n_clusters=n_regimes, random_state=random_state, n_init=20)
        regime_labels = kmeans.fit_predict(X_scaled)
        
        daily_summary['regime'] = regime_labels
        
        # Regime í†µê³„
        print(f"\nâœ“ Regime ë¼ë²¨ë§ ì™„ë£Œ")
        print(f"\nRegime ë¶„í¬:")
        regime_counts = pd.Series(regime_labels).value_counts().sort_index()
        for regime_id, count in regime_counts.items():
            pct = count / len(regime_labels) * 100
            print(f"  Regime {regime_id}: {count:,} days ({pct:.1f}%)")
        
        # Regime íŠ¹ì„±
        print(f"\nRegimeë³„ í‰ê·  íŠ¹ì„±:")
        regime_chars = daily_summary.groupby('regime')[['market_return', 'market_volatility', 
                                                         'vix', 'treasury_10y', 'yield_spread']].mean()
        print(regime_chars.round(4))
        
        # Panelì— ë³‘í•©
        print(f"\nPanel ë°ì´í„°ì— regime ë³‘í•© ì¤‘...")
        self.panel_data = self.panel_data.join(daily_summary[['regime']], how='left')
        self.regimes = daily_summary
        
        print(f"âœ“ Regime ë³‘í•© ì™„ë£Œ")
        
        # ì €ì¥
        regime_path = os.path.join(self.output_dir, 'market_regimes.csv')
        daily_summary.to_csv(regime_path)
        
        # Regime metadata
        regime_metadata = {
            'n_regimes': n_regimes,
            'clustering_features': features,
            'random_state': random_state,
            'regime_distribution': regime_counts.to_dict(),
            'regime_characteristics': regime_chars.to_dict('index')
        }
        
        metadata_path = os.path.join(self.output_dir, 'regime_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(regime_metadata, f, indent=2, default=str)
        
        print(f"\nâœ“ Regime ë°ì´í„° ì €ì¥: {regime_path}")
        print(f"âœ“ Metadata ì €ì¥: {metadata_path}")
        
        return daily_summary
    
    def create_episodes(self, support_len=60, query_len=60, 
                       train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,
                       min_history=252, purity_threshold=0.8, 
                       regime_pure=True):
        """
        Meta-Learning Episodes ìƒì„±
        
        Args:
            support_len: Support set ê¸¸ì´ (days)
            query_len: Query set ê¸¸ì´ (days)
            train_ratio: Training set ë¹„ìœ¨
            val_ratio: Validation set ë¹„ìœ¨
            test_ratio: Test set ë¹„ìœ¨
            min_history: ìµœì†Œ ì´ë ¥ (warm-up)
            purity_threshold: Regime purity ì„ê³„ê°’ (0.8 = 80%)
            regime_pure: Trueë©´ support/queryê°€ ê°™ì€ regimeì´ê³  purity > thresholdì¸ ê²ƒë§Œ
        
        Returns:
            episodes: List of episode dictionaries
        """
        print("\n" + "="*80)
        print("Meta-Learning Episodes ìƒì„±")
        print("="*80)
        
        print(f"\nì„¤ì •:")
        print(f"  - Support length: {support_len} days")
        print(f"  - Query length: {query_len} days")
        print(f"  - Episode length: {support_len + query_len} days")
        print(f"  - Min history: {min_history} days (warm-up)")
        print(f"  - Train/Val/Test: {train_ratio}/{val_ratio}/{test_ratio}")
        print(f"  - Regime-pure: {regime_pure}")
        if regime_pure:
            print(f"  - Purity threshold: {purity_threshold} ({purity_threshold*100}%)")
        
        df = self.panel_data
        
        # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
        unique_dates = sorted(df.index.unique())
        print(f"\nì´ ê±°ë˜ì¼: {len(unique_dates)} days")
        
        # Episode ì‹œì‘ ê°€ëŠ¥ ë‚ ì§œ
        start_idx = min_history + support_len
        end_idx = len(unique_dates) - query_len
        
        available_starts = unique_dates[start_idx:end_idx]
        print(f"Episode ìƒì„± ê°€ëŠ¥ ë‚ ì§œ: {len(available_starts)} days")
        print(f"  ì²« ê°€ëŠ¥ ë‚ ì§œ: {available_starts[0]}")
        print(f"  ë§ˆì§€ë§‰ ê°€ëŠ¥ ë‚ ì§œ: {available_starts[-1]}")
        
        # Train/Val/Test split (ì‹œê°„ ìˆœì„œ ìœ ì§€)
        n_total = len(available_starts)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        n_test = n_total - n_train - n_val
        
        train_dates = available_starts[:n_train]
        val_dates = available_starts[n_train:n_train+n_val]
        test_dates = available_starts[n_train+n_val:]
        
        print(f"\nSplit ê²°ê³¼:")
        print(f"  Train: {len(train_dates)} slots ({train_dates[0]} ~ {train_dates[-1]})")
        print(f"  Val:   {len(val_dates)} slots ({val_dates[0]} ~ {val_dates[-1]})")
        print(f"  Test:  {len(test_dates)} slots ({test_dates[0]} ~ {test_dates[-1]})")
        
        # Episodes ìƒì„±
        def create_episode_list(date_list, split_name):
            episodes = []
            skipped = 0
            
            for start_date in tqdm(date_list, desc=f"{split_name} episodes"):
                # Support set
                start_idx_local = unique_dates.index(start_date) - support_len
                support_dates = unique_dates[start_idx_local:start_idx_local + support_len]
                
                # Query set
                query_start_idx = start_idx_local + support_len
                query_dates = unique_dates[query_start_idx:query_start_idx + query_len]
                
                # Regime ì •ë³´
                support_mask = df.index.isin(support_dates)
                query_mask = df.index.isin(query_dates)
                
                support_regimes = df.loc[support_mask, 'regime'].dropna()
                query_regimes = df.loc[query_mask, 'regime'].dropna()
                
                if len(support_regimes) == 0 or len(query_regimes) == 0:
                    skipped += 1
                    continue
                
                sup_mode = support_regimes.mode()[0]
                qry_mode = query_regimes.mode()[0]
                
                # Regime-pure ì¡°ê±´ ì²´í¬
                if regime_pure:
                    sup_purity = (support_regimes == sup_mode).mean()
                    qry_purity = (query_regimes == qry_mode).mean()
                    
                    # Supportì™€ Queryê°€ ë‹¤ë¥¸ regimeì´ê±°ë‚˜ purityê°€ ë‚®ìœ¼ë©´ ì œì™¸
                    if sup_mode != qry_mode:
                        skipped += 1
                        continue
                    if sup_purity < purity_threshold or qry_purity < purity_threshold:
                        skipped += 1
                        continue
                
                # Row indices (PyTorch Datasetìš©)
                support_row_ids = df.loc[support_mask, 'row_id'].tolist()
                query_row_ids = df.loc[query_mask, 'row_id'].tolist()
                
                episode = {
                    'episode_id': len(episodes),
                    'split': split_name,
                    'support_start': str(support_dates[0]),
                    'support_end': str(support_dates[-1]),
                    'query_start': str(query_dates[0]),
                    'query_end': str(query_dates[-1]),
                    'support_regime': int(sup_mode),
                    'query_regime': int(qry_mode),
                    'support_row_ids': support_row_ids,
                    'query_row_ids': query_row_ids,
                    'n_support_days': len(support_dates),
                    'n_query_days': len(query_dates),
                    'n_support_points': len(support_row_ids),
                    'n_query_points': len(query_row_ids)
                }
                
                episodes.append(episode)
            
            return episodes, skipped
        
        print("\nEpisode ë¦¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        train_episodes, train_skipped = create_episode_list(train_dates, 'train')
        val_episodes, val_skipped = create_episode_list(val_dates, 'val')
        test_episodes, test_skipped = create_episode_list(test_dates, 'test')
        
        all_episodes = train_episodes + val_episodes + test_episodes
        
        print(f"\nâœ“ ì´ {len(all_episodes)} episodes ìƒì„± ì™„ë£Œ")
        print(f"  - Train: {len(train_episodes)} (skipped: {train_skipped})")
        print(f"  - Val: {len(val_episodes)} (skipped: {val_skipped})")
        print(f"  - Test: {len(test_episodes)} (skipped: {test_skipped})")
        
        # Regime ë¶„í¬
        print(f"\nRegime ë¶„í¬ (support set ê¸°ì¤€):")
        episodes_df = pd.DataFrame(all_episodes)
        for split in ['train', 'val', 'test']:
            split_df = episodes_df[episodes_df['split'] == split]
            regime_dist = split_df['support_regime'].value_counts().sort_index()
            print(f"  {split.capitalize()}:")
            for regime, count in regime_dist.items():
                pct = count / len(split_df) * 100
                print(f"    Regime {regime}: {count} ({pct:.1f}%)")
        
        # ì €ì¥
        episodes_path = os.path.join(self.output_dir, 'episodes.json')
        with open(episodes_path, 'w') as f:
            json.dump(all_episodes, f, indent=2)
        print(f"\nâœ“ Episodes ì €ì¥: {episodes_path}")
        
        # Metadata
        metadata = {
            'support_len': support_len,
            'query_len': query_len,
            'episode_len': support_len + query_len,
            'min_history': min_history,
            'regime_pure': regime_pure,
            'purity_threshold': purity_threshold if regime_pure else None,
            'n_train': len(train_episodes),
            'n_val': len(val_episodes),
            'n_test': len(test_episodes),
            'n_total': len(all_episodes),
            'n_skipped': {
                'train': train_skipped,
                'val': val_skipped,
                'test': test_skipped
            },
            'date_range': {
                'train': (str(train_dates[0]), str(train_dates[-1])),
                'val': (str(val_dates[0]), str(val_dates[-1])),
                'test': (str(test_dates[0]), str(test_dates[-1]))
            }
        }
        
        metadata_path = os.path.join(self.output_dir, 'episodes_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"âœ“ Metadata ì €ì¥: {metadata_path}")
        
        self.episodes = all_episodes
        return all_episodes
    
    def create_ticker_subsets(self, n_assets_list=[10, 50, 100, 200, 400], seed=42):
        """
        N=10, 50, 100, 200, 400 ì‹¤í—˜ì„ ìœ„í•œ ticker subset ìƒì„±
        
        ë™ì¼ episode êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë˜, tickerë§Œ subsetìœ¼ë¡œ í•„í„°ë§
        """
        print("\n" + "="*80)
        print("Ticker Subsets ìƒì„± (N-asset experiments)")
        print("="*80)
        
        all_tickers = sorted(self.panel_data['ticker'].unique())
        print(f"\nì „ì²´ í‹°ì»¤ ìˆ˜: {len(all_tickers)}")
        
        np.random.seed(seed)
        
        subsets = {}
        for n in n_assets_list:
            if n > len(all_tickers):
                print(f"  âš  N={n}ì€ ì „ì²´ í‹°ì»¤ ìˆ˜({len(all_tickers)})ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ìŠ¤í‚µ.")
                continue
            
            # ëœë¤ ìƒ˜í”Œë§
            selected = np.random.choice(all_tickers, size=n, replace=False).tolist()
            subsets[f'N{n}'] = sorted(selected)
            print(f"  âœ“ N={n}: {len(selected)}ê°œ í‹°ì»¤ ì„ íƒ")
        
        # ì „ì²´ ì„¸íŠ¸
        subsets['N_all'] = all_tickers
        print(f"  âœ“ N=all: {len(all_tickers)}ê°œ í‹°ì»¤ (ì „ì²´)")
        
        # ì €ì¥
        subsets_path = os.path.join(self.output_dir, 'ticker_subsets.json')
        with open(subsets_path, 'w') as f:
            json.dump(subsets, f, indent=2)
        print(f"\nâœ“ Ticker subsets ì €ì¥: {subsets_path}")
        
        # Subset ë©”íƒ€ë°ì´í„°
        subset_metadata = {
            'seed': seed,
            'n_assets_list': n_assets_list,
            'total_tickers': len(all_tickers),
            'subset_sizes': {k: len(v) for k, v in subsets.items()}
        }
        
        metadata_path = os.path.join(self.output_dir, 'ticker_subsets_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(subset_metadata, f, indent=2)
        print(f"âœ“ Subset metadata ì €ì¥: {metadata_path}")
        
        return subsets
    
    def save_processed_panel(self):
        """ì „ì²˜ë¦¬ëœ panel ë°ì´í„° ì €ì¥"""
        print("\n" + "="*80)
        print("ì „ì²˜ë¦¬ëœ Panel ë°ì´í„° ì €ì¥")
        print("="*80)
        
        output_path = os.path.join(self.output_dir, 'sp500_panel_processed.csv')
        self.panel_data.to_csv(output_path)
        
        print(f"âœ“ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"  - Shape: {self.panel_data.shape}")
        print(f"  - Columns: {list(self.panel_data.columns)}")
        print(f"  - Size: {os.path.getsize(output_path) / 1024**2:.1f} MB")
        
        return output_path
    
    def generate_summary_report(self):
        """ì „ì²˜ë¦¬ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*80)
        print("ì „ì²˜ë¦¬ ìš”ì•½ ë¦¬í¬íŠ¸")
        print("="*80)
        
        report = {
            'dataset': {
                'n_tickers': self.panel_data['ticker'].nunique(),
                'n_total_points': len(self.panel_data),
                'date_range': (str(self.panel_data.index.min()), str(self.panel_data.index.max())),
                'n_features': 39,
                'n_days': len(self.panel_data.index.unique())
            },
            'regimes': {
                'n_regimes': int(self.panel_data['regime'].nunique()),
                'distribution': self.panel_data.groupby('regime').size().to_dict()
            },
            'episodes': {
                'n_train': len([e for e in self.episodes if e['split'] == 'train']),
                'n_val': len([e for e in self.episodes if e['split'] == 'val']),
                'n_test': len([e for e in self.episodes if e['split'] == 'test']),
                'n_total': len(self.episodes)
            }
        }
        
        report_path = os.path.join(self.output_dir, 'preprocessing_summary.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nâœ“ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        print(f"\nìµœì¢… ë°ì´í„°ì…‹:")
        print(f"  - {report['dataset']['n_tickers']} tickers")
        print(f"  - {report['dataset']['n_total_points']:,} data points")
        print(f"  - {report['dataset']['n_features']} features")
        print(f"  - {report['regimes']['n_regimes']} regimes")
        print(f"  - {report['episodes']['n_total']} episodes")
        
        return report


# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    print("S&P 500 Meta-Learning Preprocessing Pipeline (Final Version)")
    print("=" * 80)
    print("\në…¼ë¬¸ ì‹¤í—˜ ì„¤ê³„:")
    print("  - 39 features + 1 target (returns)")
    print("  - K=4 market regimes")
    print("  - Regime-pure episodes (purity â‰¥ 80%)")
    print("  - Support/Query: 60/60 days")
    print("  - Train/Val/Test: 70/15/15 (time-ordered)")
    print("  - N-asset subsets: 10, 50, 100, 200, 400, all")
    
    # 1. Preprocessor ì´ˆê¸°í™”
    preprocessor = SP500MetaLearningPreprocessor(
        data_dir='./sp500_data',
        output_dir='./processed_data'
    )
    
    # 2. Panel ë°ì´í„° ë¡œë“œ
    panel = preprocessor.load_panel_data()
    
    # 3. EDA
    eda_results = preprocessor.eda_summary()
    
    # 4. Regime ë¼ë²¨ë§ (K=4, ë…¼ë¬¸ê³¼ ì¼ì¹˜)
    regimes = preprocessor.label_regimes(n_regimes=4, random_state=42)
    
    # 5. Episodes ìƒì„± (regime-pure, purity=0.8)
    episodes = preprocessor.create_episodes(
        support_len=60,
        query_len=60,
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
        min_history=252,
        purity_threshold=0.8,
        regime_pure=True  # ë…¼ë¬¸ì˜ ì´ë¡ ê³¼ ì¼ì¹˜
    )
    
    # 6. Ticker subsets ìƒì„±
    subsets = preprocessor.create_ticker_subsets(
        n_assets_list=[10, 50, 100, 200, 400],
        seed=42
    )
    
    # 7. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
    preprocessor.save_processed_panel()
    
    # 8. ìš”ì•½ ë¦¬í¬íŠ¸
    report = preprocessor.generate_summary_report()
    
    print("\n" + "=" * 80)
    print("ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)
    print("\nìƒì„±ëœ íŒŒì¼:")
    print("  ğŸ“Š processed_data/sp500_panel_processed.csv       (ì „ì²˜ë¦¬ëœ íŒ¨ë„ ë°ì´í„°)")
    print("  ğŸ¯ processed_data/market_regimes.csv              (ì¼ë³„ regime ë¼ë²¨)")
    print("  ğŸ“ processed_data/regime_metadata.json            (Regime ë©”íƒ€ë°ì´í„°)")
    print("  ğŸ“š processed_data/episodes.json                   (Episode ë¦¬ìŠ¤íŠ¸)")
    print("  ğŸ“‹ processed_data/episodes_metadata.json          (Episode ë©”íƒ€ë°ì´í„°)")
    print("  ğŸ² processed_data/ticker_subsets.json             (N-asset subsets)")
    print("  ğŸ“Š processed_data/ticker_subsets_metadata.json    (Subset ë©”íƒ€ë°ì´í„°)")
    print("  ğŸ“„ processed_data/preprocessing_summary.json      (ì „ì²´ ìš”ì•½)")
    
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  âœ… PyTorch Dataset êµ¬í˜„")
    print("  âœ… Meta-learner (MAML/Reptile) êµ¬í˜„")
    print("  âœ… Baseline ëª¨ë¸ (Markowitz, EW, RP) êµ¬í˜„")
    print("  âœ… ì‹¤í—˜ ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„")
