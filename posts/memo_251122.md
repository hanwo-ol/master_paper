---
title: "실험해야 할 목록"
date: 2025-11-21       
description: "졸논용 실험"
categories: [MetaLearning, Finance, Quant, Portfolio, Review]
author: "김한울"
---

# what to test in experiments, based on the methods/theory section

## 2. 나중에 실험 섹션에서 “반드시” 다뤄야 할 내용 정리

이제 핵심: 이 Method/이론 파트에서 이미 사실처럼 이야기한 것 중, 실험 섹션에서 반드시 확인·검증해줘야 할 부분들을 스텝별로 모아서 적을게요.

### 2.1. 전체 파이프라인 수준

1. **Unified pipeline의 효과**

   * “각 스텝은 개별로도 유용하지만, 상호작용이 핵심”이라고 했으므로:

     * Baseline 비교:

       * 단순 MVO (sample covariance, no deep learning)
       * Deep return model + naive risk model (no HRP, no shrinkage)
       * HRP only + simple model
       * U-Net only (no MAML, no shrinkage)
       * MAML only vs no meta-learning
       * Shrinkage only vs raw covariance
     * 최종 unified pipeline이 이들 대비 얼마나 성능·안정성을 개선하는지 (Sharpe, turnover, drawdown, etc.) 표로 보여줘야 함.

2. **Challenges 1–3 완화 여부**

   * Challenge 1 (high dimension): conditioning, eigenvalue, weight stability.
   * Challenge 2 (non-stationarity): regime별 성능, regime change 이후 적응 속도.
   * Challenge 3 (regime uncertainty): posterior noise 혹은 misclassification 상황에서도 성능이 크게 깨지지 않는지.

---

### 2.2. Step 1 (HRP Ordering) 관련 실험 TODO

이론/서술에서 주장한 것들:

* HRP ordering이 **effective dimensionality 감소**와 **covariance conditioning 개선**에 기여.
* HRP dendrogram의 cluster topology가 **regime 간 비교적 stability**를 가진다.
* 이 구조로 인해 U-Net 학습이 **sample efficient**해진다.

⇒ 실험에서 필요한 것:

1. **Covariance 구조 분석**

   * HRP ordering 전/후 covariance matrix의

     * 최소 고유값 (\lambda_{\min}),
     * condition number,
     * off-diagonal sparsity/clusteredness (예: block-diagonal approximation error)
       비교 그래프 or 표.

2. **Regime 간 cluster topology의 안정성**

   * 각 regime(또는 rolling window) 별로 HRP dendrogram을 다시 만들고,
   * tree distance 혹은 cophenetic correlation으로 topology 변화를 계량화:

     * “In our data, average cophenetic correlation between regimes is X” 같은 결과.

3. **Ablation: HRP vs 다른 ordering**

   * Random ordering / sector-group-by ordering / HRP ordering 비교:

     * 동일 U-Net 구조에서 out-of-sample 성능 (Sharpe, IC 등) 비교.
   * 여기서 “sample efficiency”: train data length를 줄여가면서(예: 3y vs 5y vs 10y) 성능 유지 정도 비교.

---

### 2.3. Step 2 (U-Net + Attention) 관련 실험 TODO

서술에서 나온 주장들:

* HRP와 결합된 U-Net이 **multi-scale** + **regime-robust** feature를 학습한다.
* Skip connections가 LSTM/GRU보다 **정보 손실이 적다**는 heuristic (Proposition 참조).
* Self-attention bottle-neck이 글로벌 의존성 및 regime shift 대응에 중요하다.

⇒ 실험에서 필요한 것:

1. **Architecture Ablation**

   * HRP + U-Net vs HRP + simple CNN vs HRP + LSTM/GRU vs (optional) Transformer baseline:

     * 예측 성능 (IC / MSE / portfolio metrics) 비교.
2. **Attention Ablation**

   * U-Net without attention vs with attention;
   * (\alpha) gate를 0/1 근처로 고정했을 때 vs 학습했을 때 성능 비교.
3. **정보 보존 관련 지표**

   * 아주 복잡한 이론적 지표가 아니어도,

     * gradient norm propagation,
     * feature variance preservation across layers,
       같은 simple한 통계량을 통해 “정보가 더 잘 전달된다”는 정성적 evidence를 조금만 보여주면 Prop.~\ref{prop:info_heuristic}와 연결되기 좋음.

---

### 2.4. Step 3 (Soft Task-Weighted MAML) 관련 실험 TODO

서술에서 사실처럼 나오는 것들:

* Regime 간 gradient alignment가 낮아서 pooled training이 suboptimal.
* MAML이 그런 상황에서 query-loss를 (O(1/K)) 수준으로 개선.
* Soft posterior weighting이 hard assignment보다 **posterior noise에 robust**하고, 업데이트가 1-Lipschitz.
* Meta-initialization이 “approximate Chebyshev center” 역할을 해서, few steps로 빠르게 적응한다.

⇒ 실험에서 필요한 것:

1. **Gradient Alignment Empirical Check**

   * 실제로 Regime별 loss gradient를 추정해서:

     * pairwise cosine similarity 분포 (mean, variance, histogram).
   * 이게 “low or negative alignment”라는 claim을 뒷받침해야 함.

2. **Meta-learning vs Baselines**

   * Pooled single model vs per-regime separate models vs MAML:

     * 각 regime 내/전환 구간에서 성능 (IC, Sharpe, etc.).
   * 특히 regime switch 이후 “몇 step 안에 회복되는지”를 time-series plot으로 보여주면 Theorem 설명과 잘 맞음.

3. **Soft vs Hard weighting**

   * 동일 HMM posterior로:

     * hard regime assignment (argmax) vs soft mixture (posterior weights) 비교.
   * 시나리오:

     * posterior가 애매한 구간(0.5/0.5 근처)에서 portfolio turnover, realized PnL variance 비교 → soft가 더 smooth한지.

4. **K-step inner loop sensitivity**

   * (K=0,1,2,3,...)에 대한 성능 곡선:

     * 실제로 “few steps에서 대부분의 gain”이 나는지 확인 (이론적 (1/K) trend와 qualitatively 맞는지).

---

### 2.5. Step 4 (Shrinkage + Turnover Control) 관련 실험 TODO

서술에서 사실처럼 이야기한 것:

* Ledoit–Wolf shrinkage가 (\lambda_{\min})을 올리고 dominance ratio를 줄여 weight 안정성을 크게 개선한다.
* Turnover penalty가

  * transaction cost를 proxy하고,
  * regime misclassification impact를 capped 한다.

⇒ 실험에서 필요한 것:

1. **Risk model ablation**

   * Sample covariance vs Ledoit–Wolf vs (optional) simpler targets (diagonal, factor model):

     * weight stability, realized risk(variance), realized Sharpe 비교.
   * 특히 “same (\hat{\mu})”를 쓰되, risk model만 바꿨을 때 차이를 보이면 Prop.~\ref{prop:covariance_sensitivity}와 잘 맞음.

2. **Turnover penalty sensitivity**

   * (\kappa = 0, \kappa_{\text{low}}, \kappa_{\text{mid}}, \kappa_{\text{high}}) 값에 따른:

     * turnover,
     * net performance(After-cost Sharpe),
     * weight change distribution,
       등 플롯/표.

3. **Regime misclassification 시나리오 실험**

   * posterior에 인위적으로 noise를 더해서 misclassification을 증가시켰을 때,
   * turnover penalty 유무에 따라 performance degradation이 어떻게 다른지 비교:

     * 이게 Theorem~\ref{thm:confusion_decomp} + turnover term의 역할과 직접 연결됨.

---

## 3. 요약: 통계학과 관점 + 실험 TODO 한 번에 정리

* **통계학과 관점에서**

  * 큰 틀은 괜찮고, 특히 “deep model + shrinkage MVO”의 분리, HRP의 구조적 사용, meta-learning의 gradient alignment 관점은 충분히 통계적인 이야기임.
  * 다만,

    * “rolling vs global standardization” 수식 일관성,
    * HRP ordering의 효과를 “deterministic fact”처럼 쓰지 말고 “heuristic + empirical” 톤으로,
    * MAML 관련 Theorem의 조건/결론을 Method에서 살짝만 더 정확히 요약,
    * L1 penalty가 들어간 QP의 특성(비미분 convext 문제) 정도만 정리해주면, 통계학과에서도 크게 트집 잡기 어려운 수준이 될 것 같아요.

* **실험 섹션에서 반드시 다룰 것들 (요약)**

  1. 파이프라인 전체 ablation: 각 스텝 제거/치환 시 성능과 안정성 비교.
  2. HRP: covariance conditioning 개선, cluster topology stability, HRP vs random/sector ordering 비교.
  3. U-Net + attention: architecture ablation (LSTM/Transformer/attention off), HRP 유무 비교, sample efficiency 표시.
  4. MAML: gradient alignment 측정, meta-learning vs pooled vs per-regime, soft vs hard posterior, K-step sensitivity.
  5. Shrinkage + turnover: different risk models, (\kappa) sensitivity, regime misclassification 시 turnover penalty의 버퍼 역할.

이 정도까지 커버하면, 이론 파트에서 한 말들이 “공갈”로 보이지 않고, 통계학과 리뷰어 입장에서도 “이 사람들 자기 주장에 대해 꽤 성실하게 검증했다”는 느낌을 줄 수 있을 거예요.

다음 섹션 넘어가기 전에,

* (1) 표기/톤(rolling, heuristic vs theorem)만 한 번 정리,
* (2) 위 실험 TODO 리스트 기반으로 실제 실험 설계 skeleton 을 잡아두면 좋겠습니다.
