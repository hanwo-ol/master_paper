{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503f73a2",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hypernetworks\"\n",
    "date: 2025-11-16       \n",
    "description: \"메타 러닝 관련 논문 요약 및 주요 내용\"\n",
    "categories: [MetaLearning, Review, Hypernetworks]\n",
    "author: \"김한울\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16efc7",
   "metadata": {},
   "source": [
    "```\n",
    "@inproceedings{ha2017hypernetworks,\n",
    "  title={HyperNetworks},\n",
    "  author={Ha, David and Dai, Andrew M and Le, Quoc V},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2017}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a61753",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "이 연구는 하이퍼네트워크를 탐구합니다. \n",
    "**하이퍼네트워크**는 **한 네트워크를 사용하여 다른 네트워크의 가중치를 생성하는 방식**입니다.    \n",
    "\n",
    "하이퍼네트워크는 자연에서 발견되는 것과 유사한 추상화를 제공합니다: 유전형질(하이퍼네트워크)과 표현형(주 네트워크) 간의 관계입니다. \n",
    "\n",
    "진화 알고리즘의 HyperNEAT과 유사하지만, 본 연구의 하이퍼네트워크는 역전파(backpropagation)로 End-to-End 학습되므로 일반적으로 더 빠릅니다. \n",
    "\n",
    "본 연구의 초점은 하이퍼네트워크를 심층 합성곱 신경망과 장기 순환 신경망에 유용하게 만드는 것입니다. 여기서 하이퍼네트워크는 계층 간 가중치 공유의 완화된 형태로 볼 수 있습니다. 주요 결과는 하이퍼네트워크가 LSTM의 비공유 가중치를 생성할 수 있으며, 문자 수준의 언어 모델링, 필기 생성, 신경 기계 번역을 포함한 다양한 시퀀스 모델링 작업에서 최첨단에 가까운 성능을 달성할 수 있다는 것입니다. \n",
    "\n",
    "이는 순환 신경망의 가중치 공유 패러다임에 도전합니다. 또한 결과는 합성곱 신경망에 적용된 하이퍼네트워크가 최첨단 기준 모델과 비교하여 이미지 인식 작업에서 뛰어난 결과를 달성하면서도 학습 가능한 매개변수가 더 적음을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad4771",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "본 연구에서는 작은 네트워크(하이퍼네트워크라고 불림)를 사용하여 더 큰 네트워크(주 네트워크라고 불림)의 가중치를 생성하는 방식을 고려합니다. 주 네트워크의 동작은 일반적인 신경망과 동일합니다: 원시 입력을 원하는 목표로 매핑하도록 학습합니다. 반면 하이퍼네트워크는 가중치 구조에 대한 정보를 포함하는 입력 세트를 받아서 해당 계층의 가중치를 생성합니다(그림 1 참조).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb057f",
   "metadata": {},
   "source": [
    "![Figure 1](/sources/20251116_1_figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c95c1",
   "metadata": {},
   "source": [
    "\n",
    "HyperNEAT은 하이퍼네트워크의 예이며, 여기서 입력은 주 네트워크의 각 가중치에 대한 가상 좌표 집합입니다. 본 연구에서는 입력이 주어진 계층의 전체 가중치를 설명하는 임베딩 벡터인 더 강력한 방식에 중점을 둡니다. 우리의 임베딩 벡터는 고정 매개변수가 될 수 있으며, End-to-End 학습 중에도 학습되므로 주 네트워크의 계층 내 및 계층 간 가중치 공유를 근사할 수 있습니다. 또한 우리의 임베딩 벡터는 하이퍼네트워크에 의해 동적으로 생성될 수도 있으므로, 순환 네트워크의 가중치가 시간 단계에 따라 변경되고 입력 시퀀스에 적응할 수 있습니다.\n",
    "\n",
    "우리는 다양한 맥락에서 하이퍼네트워크의 동작을 조사하기 위해 실험을 수행하고, 하이퍼네트워크가 배치 정규화 및 계층 정규화와 같은 다른 기술과 잘 혼합된다는 것을 발견했습니다. \n",
    "\n",
    "주요 결과는 하이퍼네트워크가 LSTM을 위한 비공유 가중치를 생성할 수 있으며, 이것이 표준 LSTM 버전(Hochreiter & Schmidhuber, 1997)보다 더 잘 작동한다는 것입니다. \n",
    "\n",
    "(1) Character Penn Treebank, Hutter Prize Wikipedia 데이터세트를 사용한 언어 모델링 작업에서 LSTM을 위한 하이퍼네트워크는 최첨단에 가까운 결과를 달성합니다. \n",
    "\n",
    "(2) IAM 필기 데이터세트를 사용한 필기 생성 작업에서 LSTM을 위한 하이퍼네트워크는 높은 정량적 및 정성적 결과를 달성합니다.\n",
    "\n",
    "(3) CIFAR-10을 사용한 이미지 분류에서 하이퍼네트워크가 깊은 합성곱 신경망(LeCun 외, 1990)의 가중치를 생성하는 데 사용될 때, 더 적은 학습 가능한 매개변수를 가지면서도 최첨단 모델과 비교하여 존경할만한 결과를 얻습니다. \n",
    "\n",
    "(4) 단순한 작업 외에도, 우리는 LSTM을 위한 하이퍼네트워크가 대규모의 프로덕션 수준의 신경 기계 번역 모델의 성능 증가를 제공한다는 것을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3de79e",
   "metadata": {},
   "source": [
    "# Motivation and Related Work\n",
    "\n",
    "본 연구의 접근법은 진화 컴퓨팅 방법에서 영감을 받았습니다. 진화 컴퓨팅에서는 수백만 개의 가중치 매개변수로 구성된 대규모 탐색 공간에서 직접 작업하는 것이 어렵습니다. 더 효율적인 방법은 더 작은 네트워크를 진화시켜 더 큰 네트워크의 가중치 구조를 생성하도록 하여, 훨씬 작은 가중치 공간 내에서 탐색이 제한되도록 하는 것입니다. 이 접근법의 한 사례가 HyperNEAT 프레임워크입니다.\n",
    "\n",
    "## HyperNEAT Framework\n",
    "\n",
    "HyperNEAT 프레임워크에서는 구성 패턴 생성 네트워크(Compositional Pattern-Producing Networks, CPPNs)가 진화하여 훨씬 더 큰 주 네트워크의 가중치 구조를 정의합니다. \n",
    "\n",
    "+ 본 연구의 접근법과 밀접하게 관련된 것은 HyperNEAT의 단순화된 변형으로, 구조는 고정되고 가중치는 **이산 코사인 변환(Discrete Cosine Transform, DCT)**을 통해 진화하는 **압축 가중치 탐색(Compressed Weight Search)**입니다. \n",
    "+ 본 연구의 접근법과 더욱 밀접하게 관련된 것은 구조는 진화하지만 가중치는 학습되는 미분 가능한 패턴 생성 네트워크(Differentiable Pattern Producing Networks, DPPNs)와 선형 계층이 DCT로 압축되고 매개변수가 학습되는 ACDC-Networks입니다. \n",
    "\n",
    "그러나 이러한 방법을 사용한 대부분의 보고된 결과는 소규모 수준에 그쳤는데, 아마도 학습이 느리고 효율적이기 위해 휴리스틱이 필요하기 때문일 것입니다.\n",
    "\n",
    "## This Paper vs. HyperNEAT\n",
    "\n",
    "본 연구의 접근법과 HyperNEAT의 주요 차이점은 본 연구의 하이퍼네트워크가 주 네트워크와 함께 그래디언트 하강법을 사용하여 End-to-End 학습되므로 더 효율적이라는 것입니다.\n",
    "\n",
    "그래디언트 하강법을 사용한 End-to-End 학습 외에도, 본 연구의 접근법은 모델 유연성과 학습 단순성 측면에서 압축 가중치 탐색과 HyperNEAT 사이에서 좋은 균형을 이룹니다.\n",
    "\n",
    "+ 첫째, 압축 가중치 탐색에 사용되는 이산 코사인 변환이 너무 단순할 수 있으며 DCT 사전 지식이 많은 문제에 적합하지 않을 수 있다고 주장할 수 있습니다. \n",
    "+ 둘째, HyperNEAT가 더 유연하지만, HyperNEAT에서 아키텍처와 가중치를 모두 진화시키는 것은 대부분의 실용적인 문제에 대해 과도한 방법입니다.\n",
    "\n",
    "HyperNEAT와 DCT에 관한 연구 이전에도, Schmidhuber(1992, 1993)는 빠른 가중치(fast weights) 개념을 제안했습니다. 이 개념에서는 한 네트워크가 두 번째 네트워크에 대한 문맥 의존적 가중치 변경을 생성할 수 있습니다. 당시 피드포워드 네트워크를 위한 빠른 가중치를 입증하기 위해 소규모 실험이 수행되었지만, 아마도 현대적인 계산 도구가 부족했기 때문에 순환 네트워크 버전은 주로 사고 실험(thought experiment)으로 언급되었습니다. 후속 연구에서는 빠른 가중치의 실용적 응용을 입증했으며, 생성기 네트워크가 진화를 통해 학습되어 인공 제어 문제를 해결했습니다.\n",
    "\n",
    "## The Key Point\n",
    "\n",
    "한 네트워크가 다른 네트워크와 상호 작용하는 개념은 여러 연구의 핵심이며, 특히 합성곱 네트워크의 특정 매개변수가 다른 네트워크에 의해 예측되는 연구들이 있습니다. 그러나 이러한 연구들은 순환 네트워크에 이 접근법을 사용하는 것을 탐구하지 않았으며, 이것이 본 연구의 주요 기여입니다.\n",
    "\n",
    "**본 연구의 초점은 계층 임베딩 벡터를 입력으로 받아 합성곱 네트워크 및 순환 네트워크와 같은 실용적인 아키텍처를 위한 가중치를 생성하는 것**입니다. \n",
    "\n",
    "+ 그러나 본 연구의 하이퍼네트워크는 DPPNs와 유사하게 좌표 정보를 입력으로 받아 완전 연결 네트워크의 가중치를 생성하는 데도 활용될 수 있습니다. 이 설정을 사용하면, 하이퍼네트워크는 명시적으로 지시받지 않고도 합성곱 아키텍처를 근사적으로 복원할 수 있으며, 이는 진화에 의한 합성곱(Convolution by Evolution)에서 얻은 것과 유사한 결과입니다. 이 결과는 부록 A.1에 설명되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05617893",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddaad48f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c516e1f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d869a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393a55f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e813b36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "441d3feb",
   "metadata": {},
   "source": [
    "# Paper Appendix\n",
    "\n",
    "논문의 실제 Appendix를 위한 공간입니다. 다음 장인 My Appendix는 제가 공부하면서 정리한 Appendix입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ca0b3",
   "metadata": {},
   "source": [
    "# My Appendix 1\n",
    "## What is Evolutionary Computing?\n",
    "\n",
    "**Evolutionary Computing**은 생물학적 진화 과정에서 영감을 받은 최적화 알고리즘의 한 분야입니다. 자연 선택, 유전, 돌연변이 같은 자연의 진화 메커니즘을 컴퓨터 상에서 모방하여 복잡한 문제를 해결하는 인공지능 및 소프트 컴퓨팅의 하위 분야입니다.\n",
    "\n",
    "### 핵심 개념과 작동 원리\n",
    "\n",
    "진화 연산은 **개체군 기반(population-based)** 탐색 방식을 사용합니다. 후보 솔루션들(개체)을 모집단으로 구성하고, 각 개체의 적합도(fitness)를 평가한 후, 우수한 개체를 선택하여 교배(crossover)와 돌연변이(mutation)를 통해 새로운 세대를 생성합니다. 이 과정을 반복하면서 점진적으로 더 나은 솔루션으로 진화해 나갑니다.\n",
    "\n",
    "주요 구성 요소는 다음과 같습니다:\n",
    "\n",
    "- **선택(Selection)**: 적합도가 높은 개체를 우선적으로 선택\n",
    "- **교배(Crossover/Recombination)**: 부모 개체의 정보를 결합하여 자손 생성\n",
    "- **돌연변이(Mutation)**: 무작위 변화를 도입하여 다양성 확보\n",
    "- **적합도 함수(Fitness Function)**: 솔루션의 품질을 평가하는 기준\n",
    "\n",
    "### 주요 알고리즘 종류\n",
    "\n",
    "진화 연산은 여러 알고리즘 계열을 포함합니다:\n",
    "\n",
    "- **유전 알고리즘(Genetic Algorithms, GA)**: 가장 널리 사용되는 방법\n",
    "- **진화 전략(Evolution Strategies, ES)**: 실수형 최적화에 특화\n",
    "- **진화 프로그래밍(Evolutionary Programming, EP)**: 유한 상태 기계 진화\n",
    "- **유전 프로그래밍(Genetic Programming, GP)**: 프로그램 구조 자체를 진화\n",
    "- **군집 지능(Swarm Intelligence)**: 개미 군집 최적화(ACO), 입자 군집 최적화(PSO) 등\n",
    "\n",
    "### 연구 분야로서의 특징\n",
    "\n",
    "진화 연산은 다음과 같은 특징으로 인해 광범위하게 활용됩니다:\n",
    "\n",
    "- **전역 최적화**: 지역 최적점(local optima)에 덜 빠지고 전역 해를 찾을 수 있음\n",
    "- **문제 독립성**: 문제에 대한 사전 가정이 거의 필요 없음\n",
    "- **병렬화 가능성**: 개체군 기반 특성으로 대규모 병렬 처리에 적합\n",
    "- **복잡한 문제 해결**: 방대한 해공간과 비선형성을 가진 문제에 효과적\n",
    "\n",
    "### 기계학습과의 결합\n",
    "\n",
    "진화 연산은 기계학습, 특히 신경망 분야와 깊이 결합되고 있습니다:\n",
    "\n",
    "- **신경망 구조 탐색(Neural Architecture Search, NAS)**: 최적의 신경망 구조를 자동으로 설계\n",
    "- **뉴로진화(Neuroevolution)**: 신경망의 가중치, 구조, 하이퍼파라미터를 진화\n",
    "- **AutoML**: 기계학습 모델과 파이프라인을 자동으로 최적화\n",
    "\n",
    "## 최신 연구 동향 (2024-2025)\n",
    "\n",
    "### 1. 뉴로진화와 신경망 최적화\n",
    "\n",
    "**대규모 심층 신경망 최적화**가 주요 연구 주제입니다. 2024-2025년 연구들은 진화 알고리즘을 사용하여 심층 신경망의 구조와 하이퍼파라미터를 최적화하는 방법을 탐구하고 있습니다.\n",
    "\n",
    "- **\"Evolving Neural Architectures: A Genetic Algorithm Approach to Deep Learning Optimization\"** (2024): 유전 알고리즘을 사용한 신경망 구조 최적화 연구로, 적응적 개체군 제어, 다양성 보존 돌연변이, 하이브리드 강화학습 전략을 제안했습니다. 병렬 처리와 가중치 공유를 통해 계산 부담을 줄이는 생물학적 영감 접근법을 도입했습니다.\n",
    "\n",
    "- **GECCO 2025 - Neuroevolution at Work Workshop**: 2025년 주요 학회에서는 뉴로진화와 NAS의 통합이 핵심 주제입니다. 파라미터 공간 다양성 부족 문제와 계산 효율성 향상이 주요 도전 과제로 제기되고 있습니다.\n",
    "\n",
    "### 2. GPU 가속 진화 알고리즘\n",
    "\n",
    "**텐서화(Tensorization)를 활용한 GPU 가속**이 중요한 트렌드입니다:\n",
    "\n",
    "- **\"GPU-accelerated Evolutionary Multiobjective Optimization Using Tensorized RVEA\"** (GECCO 2024): GPU를 활용한 대규모 다목적 최적화\n",
    "- **\"Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration\"** (GECCO 2024): NEAT 알고리즘의 GPU 가속 버전\n",
    "- **\"Tensorized Ant Colony Optimization for GPU Acceleration\"** (GECCO 2024): 개미 군집 최적화의 GPU 병렬화\n",
    "\n",
    "### 3. 협력적 공진화(Cooperative Co-evolution)\n",
    "\n",
    "**고차원 문제 해결을 위한 협력적 공진화** 연구가 활발합니다:[8]\n",
    "\n",
    "- **\"PyCCEA: A Python package of cooperative co-evolutionary algorithms for feature selection\"** (2025): 고차원 특징 선택 문제를 위한 협력적 공진화 프레임워크를 제공합니다. 문제를 여러 하위 구성요소로 분할하여 각각 독립적으로 진화시키는 방식입니다.\n",
    "\n",
    "### 4. 진화와 학습의 시너지\n",
    "\n",
    "**진화와 학습의 결합**이 주목받고 있습니다:\n",
    "\n",
    "- **\"Neuroevolution insights into biological neural computation\"** (Science, 2025년 2월): 진화가 신경 회로 구조를 어떻게 형성하는지, 그리고 진화와 학습이 어떻게 협력하는지에 대한 종합적 리뷰. 진화는 신경망 구조를 최적화하고, 학습은 개체가 생애 동안 적응할 수 있게 합니다.\n",
    "\n",
    "- 연구 결과에 따르면, 진화된 신경망은 모듈성(modularity), 전문화된 제어 메커니즘(command neurons), 효율적 학습을 위한 구조적 특성을 자연스럽게 발달시킵니다.\n",
    "\n",
    "### 5. 분산 진화 신경망\n",
    "\n",
    "**빅데이터를 위한 분산 처리 프레임워크**가 발전하고 있습니다:\n",
    "\n",
    "- **\"A novel neural network model with distributed evolutionary algorithm\"** (Nature, 2023): Apache Spark 프레임워크를 사용한 분산 유전 알고리즘 기반 신경망 학습. 대용량 데이터에서 전통적 방법 대비 80% 이상의 계산 시간 개선을 달성했습니다.\n",
    "\n",
    "### 6. 최신 응용 분야\n",
    "\n",
    "2024-2025년 연구들은 다양한 실제 응용에 진화 연산을 적용하고 있습니다:\n",
    "\n",
    "- **재료 과학**: Neuroevolution Potential (NEP) 접근법이 분자 동역학 시뮬레이션에서 탁월한 정확도와 계산 효율성을 보이고 있습니다.\n",
    "- **음악 분류**: 차등 진화 알고리즘을 사용한 CNN 파라미터 최적화\n",
    "- **의료 진단**: 암 스크리닝을 위한 앙상블 자연 영감 알고리즘\n",
    "- **로봇공학**: 자가 조직화 입자 시스템의 집단 행동 진화\n",
    "- **공기역학**: 심층학습과 유전 알고리즘을 결합한 공기역학적 설계\n",
    "\n",
    "### 7. 주요 학술 컨퍼런스 및 저널\n",
    "\n",
    "진화 연산 분야의 최신 연구는 다음 학술 장소에서 발표됩니다:\n",
    "\n",
    "- **GECCO (Genetic and Evolutionary Computation Conference)**: 가장 권위 있는 학회\n",
    "- **Evolutionary Computation (MIT Press)**: 최고 수준의 저널\n",
    "- **Swarm and Evolutionary Computation**: 전문 저널\n",
    "- **Nature**, **Science**: 최근 뉴로진화 관련 리뷰 논문 게재\n",
    "\n",
    "진화 연산은 생물학적 원리를 컴퓨터 과학에 적용하여 복잡한 최적화 문제를 해결하는 강력한 도구로, 현재 인공지능, 특히 신경망 설계와 최적화 분야에서 혁신적인 돌파구를 제공하고 있습니다.\n",
    "\n",
    "[1](https://www.sciencedirect.com/topics/computer-science/evolutionary-computation)\n",
    "[2](https://en.wikipedia.org/wiki/Evolutionary_computation)\n",
    "[3](https://github.com/Evolutionary-Intelligence/DistributedEvolutionaryComputation)\n",
    "[4](https://nn.cs.utexas.edu/?stanley%3Anaturemi19)\n",
    "[5](https://gecco-2025.sigevo.org/Workshop?itemId=2341)\n",
    "[6](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5115996)\n",
    "[7](https://arxiv.org/list/cs.NE/2024-04)\n",
    "[8](https://joss.theoj.org/papers/10.21105/joss.08348.pdf)\n",
    "[9](https://www.cognizant.com/us/en/ai-lab/blog/neuroevolution-insights)\n",
    "[10](https://www.science.org/doi/10.1126/science.adp7478)\n",
    "[11](https://www.nature.com/articles/s41598-023-37540-z)\n",
    "[12](https://pubs.aip.org/aip/cpr/article/6/1/011310/3340020/Advances-in-modeling-complex-materials-The-rise-of)\n",
    "[13](https://arxiv.org/abs/2501.11191)\n",
    "[14](https://dl.acm.org/doi/10.1016/j.asoc.2024.111262)\n",
    "[15](https://www.sciencedirect.com/science/article/abs/pii/S0045782524004432)\n",
    "[16](https://direct.mit.edu/evco)\n",
    "[17](https://www.sciencedirect.com/journal/swarm-and-evolutionary-computation)\n",
    "[18](https://hexaware.com/glossary/evolutionary-computation/)\n",
    "[19](https://tavtechsolutions.com/glossary/evolutionary-computation/)\n",
    "[20](https://ceng.metu.edu.tr/evolutionary-computing-group)\n",
    "[21](https://www.walshmedicalmedia.com/open-access/evolutionary-computation-adapting-biological-processes-for-algorithmic-innovation.pdf)\n",
    "[22](https://gecco-2025.sigevo.org/Workshop?itemId=2338)\n",
    "[23](https://onlinelibrary.wiley.com/doi/10.1002/9781118534823.ch6)\n",
    "[24](https://dl.acm.org/doi/10.1145/3712256.3726318)\n",
    "[25](https://www.igi-global.com/dictionary/evolutionary-computing/39414)\n",
    "[26](https://www.sciencedirect.com/science/article/pii/S2773064625000064)\n",
    "[27](https://ieeexplore.ieee.org/document/819926)\n",
    "[28](http://ieeexplore.ieee.org/document/572107/)\n",
    "[29](https://taylorandfrancis.com/knowledge/Engineering_and_technology/Artificial_intelligence/Evolutionary_computation/)\n",
    "[30](https://ieeexplore.ieee.org/document/10612194/)\n",
    "[31](https://www.sciencedirect.com/science/article/abs/pii/S0957417423032530)\n",
    "[32](https://www.arxiv.org/list/cs.NE/2024-12?skip=25&show=2000)\n",
    "[33](https://arxiv.org/abs/2506.10184)\n",
    "[34](https://pubs.aip.org/aip/acp/article/3021/1/060035/3280120/Evolutionary-algorithm-for-automated-formation-of)\n",
    "[35](https://ieeexplore.ieee.org/document/10576350/)\n",
    "[36](https://dl.acm.org/doi/10.1145/2598394.2602287)\n",
    "[37](https://www.sciencedirect.com/science/article/abs/pii/S1386947725001936)\n",
    "[38](https://www.nature.com/articles/s41598-025-06375-1)\n",
    "[39](https://dl.acm.org/doi/pdf/10.1145/3712256.3726457)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969097f",
   "metadata": {},
   "source": [
    "# My Appendix 2\n",
    "## HyperNetworks와 Bayesian Meta-Learning, Uncertainty의 결합\n",
    "\n",
    "### 1. HyperNetworks의 Bayesian 결합 가능 요소\n",
    "\n",
    "#### 1.1 가중치 생성 메커니즘의 확률적 해석\n",
    "\n",
    "**HyperNetworks의 핵심 구조**가 Bayesian 프레임워크와 자연스럽게 결합됩니다:[1][2]\n",
    "\n",
    "논문에서 하이퍼네트워크는 임베딩 벡터 $$z$$를 받아 가중치 $$W$$를 생성합니다. 이를 Bayesian 관점에서 재해석하면:[3]\n",
    "\n",
    "- **Point estimate** (원논문): $$W = f_{\\theta}(z)$$\n",
    "- **Bayesian 확장**: $$W \\sim p(W|z, \\theta)$$로 가중치의 **사후 분포(posterior distribution)**를 모델링[2][1][4]\n",
    "\n",
    "**장점**: \n",
    "- 태스크별 가중치의 불확실성을 명시적으로 모델링\n",
    "- 데이터가 적은 few-shot 상황에서 epistemic uncertainty 포착[2][1]\n",
    "\n",
    "#### 1.2 동적 임베딩의 불확실성\n",
    "\n",
    "논문의 **동적 하이퍼네트워크**는 입력에 따라 임베딩이 변하는데, 이는 Bayesian 관점에서 매우 풍부한 연구 주제입니다:[5][3]\n",
    "\n",
    "- 임베딩 자체에 불확실성 부여: $$z \\sim p(z|\\text{task})$$[6][5]\n",
    "- **Bayesian meta-prompt** 개념과 연결[5]\n",
    "- 태스크 표현의 불확실성이 가중치 불확실성으로 전파[4][7]\n",
    "\n",
    "#### 1.3 계층별 가중치 스케일링\n",
    "\n",
    "논문의 **weight scaling vectors** 접근법:[3]\n",
    "```\n",
    "W(d(z)) = W ⊙ d(z)\n",
    "```\n",
    "\n",
    "이는 Bayesian 관점에서:\n",
    "- $$d(z)$$를 확률 변수로 확장 가능\n",
    "- 전체 가중치 행렬보다 **낮은 차원에서 불확실성 모델링**\n",
    "- 계산 효율적인 Variational Inference 가능[1][2]\n",
    "\n",
    "### 2. 최신 연구: BayesianHMAML (2022)\n",
    "\n",
    "**Hypernetwork approach to Bayesian MAML**이 대표적인 사례입니다.[2][1]\n",
    "\n",
    "#### 2.1 핵심 아이디어\n",
    "\n",
    "**기존 MAML의 한계**:\n",
    "- 그래디언트 기반 업데이트만 사용\n",
    "- 불확실성 정량화 불가\n",
    "- Few-shot 상황에서 과적합 위험[1][2]\n",
    "\n",
    "**BayesianHMAML의 해결책**:[2][1]\n",
    "```\n",
    "(μ(Si), σ(Si)) = Hφ(Si, fθ(Si))\n",
    "θ'i ~ N(θ + μ(Si), σ(Si))\n",
    "```\n",
    "\n",
    "하이퍼네트워크 $$H_{\\phi}$$가:\n",
    "- 서포트 세트 $$S_i$$로부터 정보 집계\n",
    "- 태스크별 가중치의 **평균(μ)과 분산(σ)** 생성[1][2]\n",
    "- 가중치를 분포에서 샘플링하여 불확실성 포착[2][1]\n",
    "\n",
    "**Decision Making 관점의 장점**:[8][2]\n",
    "- **Epistemic uncertainty 정량화**: 모델이 얼마나 확신하는지 측정\n",
    "- **Rejection diagnosis**: 불확실성이 높은 예측은 거부 가능[7]\n",
    "- **Risk-aware decision**: 불확실성을 고려한 안전한 의사결정[9]\n",
    "\n",
    "#### 2.2 실험 결과\n",
    "\n",
    "BayesianHMAML은 다음에서 우수한 성능을 보였습니다:[8][2]\n",
    "- **miniImageNet 5-way 1-shot**: 기존 MAML 대비 정확도 향상\n",
    "- **Uncertainty calibration**: 예측 불확실성이 실제 오류와 높은 상관관계\n",
    "- **Out-of-distribution detection**: OOD 샘플 탐지 능력 향상[8]\n",
    "\n",
    "### 3. 최신 연구 트렌드 (2024-2025)\n",
    "\n",
    "#### 3.1 Meta-Mol: 약물 발견을 위한 Bayesian Meta-Learning\n",
    "\n",
    "**\"Pushing the boundaries of few-shot learning for low-data drug discovery\"** (Nature Communications, 2025):[4]\n",
    "\n",
    "**문제 상황**:\n",
    "- 약물 특성 예측은 데이터 부족 문제 심각\n",
    "- 잘못된 예측은 임상 시험 실패로 이어짐\n",
    "- **Uncertainty 정량화 필수**[4]\n",
    "\n",
    "**Meta-Mol의 접근법**:[4]\n",
    "1. **Bayesian MAML 변형**: 범용 가중치(universal weights)는 point estimate, 태스크별 가중치는 사후 분포로 학습\n",
    "2. **HyperNetwork 활용**: 서포트 세트에서 사후 분포의 파라미터(평균, 분산) 직접 생성[4]\n",
    "3. **복잡한 사후 분포 학습**: 충분히 표현력 있는 하이퍼네트워크로 임의의 사후 분포 근사[4]\n",
    "\n",
    "**Decision Making에의 기여**:[4]\n",
    "- **불확실성 기반 샘플 필터링**: 신뢰도 낮은 예측 제거\n",
    "- **리스크 인지 약물 설계**: 부작용 가능성이 높은 화합물 사전 제외\n",
    "- **능동 학습(Active Learning)**: 불확실성이 높은 영역에 실험 자원 집중\n",
    "\n",
    "#### 3.2 UBMF: 결함 진단을 위한 Uncertainty-aware Bayesian Meta-Learning\n",
    "\n",
    "**\"UBMF: Uncertainty-aware bayesian meta-learning framework\"** (2025):[7]\n",
    "\n",
    "**산업 응용에서의 Uncertainty**:[7]\n",
    "- **3가지 불확실성 원천**: 미지의 조건, 적은 샘플, 도메인 외 데이터\n",
    "- 잘못된 진단은 장비 파손이나 안전 사고로 이어질 수 있음[7]\n",
    "\n",
    "**핵심 기여**:[7]\n",
    "1. **통합 불확실성 모델링**: Aleatoric과 epistemic uncertainty를 체계적으로 구분하고 정량화[7]\n",
    "2. **불확실성 기반 샘플 필터링**: OOD 샘플을 자동으로 감지하고 제거[7]\n",
    "3. **Bayesian 메타 지식 추출**: 사후 확률 보정으로 분류기 정밀도 향상[7]\n",
    "\n",
    "**Decision Making 프레임워크**:[7]\n",
    "- **신뢰도 임계값 설정**: 불확실성이 높으면 인간 전문가에게 판단 위임\n",
    "- **리스크 기반 우선순위**: 고위험 결함을 우선 처리\n",
    "- **적응적 진단**: 새로운 조건에서 메타 지식을 활용하여 빠르게 적응\n",
    "\n",
    "#### 3.3 Trust-Bayes: 신뢰할 수 있는 Uncertainty Quantification\n",
    "\n",
    "**\"Bayesian meta learning for trustworthy uncertainty quantification\"** (2024):[10][11]\n",
    "\n",
    "**문제 정의**:[10]\n",
    "- 기존 Bayesian 방법들은 모델 불확실성과 데이터 불확실성을 혼동\n",
    "- 부정확한 불확실성 추정은 잘못된 의사결정으로 이어짐[10]\n",
    "\n",
    "**Trust-Bayes 프레임워크**:[10]\n",
    "- **Trustworthy uncertainty**: 보정된(calibrated) 불확실성 제공\n",
    "- **Meta-learning과 결합**: 다양한 태스크에서 학습하여 일반화 가능한 불확실성 추정\n",
    "- **최적화 프레임워크**: 불확실성 품질을 명시적으로 최적화[10]\n",
    "\n",
    "#### 3.4 SurvUnc: 생존 분석을 위한 Meta-Model 기반 Uncertainty\n",
    "\n",
    "**\"SurvUnc: A Meta-Model Based Uncertainty Quantification Framework\"** (2024):[12][13]\n",
    "\n",
    "**의료 의사결정에서의 중요성**:[12]\n",
    "- 생존 예측의 불확실성은 치료 결정에 직접적 영향\n",
    "- 높은 불확실성 → 추가 검사 필요\n",
    "- 낮은 불확실성 → 적극적 치료 가능[12]\n",
    "\n",
    "**메타 모델 접근법**:[13][12]\n",
    "- Base survival model 위에 경량 메타 모델 구축\n",
    "- **Anchor-based learning**: Concordance 개념을 활용한 불확실성 학습[12]\n",
    "- **Post-hoc uncertainty**: 기존 모델 수정 없이 불확실성 추가[12]\n",
    "\n",
    "**평가 프로토콜**:[12]\n",
    "- **Selective prediction**: 확신도 높은 예측만 사용\n",
    "- **Misprediction detection**: 오류 가능성 사전 파악\n",
    "- **OOD detection**: 분포 외 환자 식별[12]\n",
    "\n",
    "### 4. Decision Making에서 Uncertainty의 역할\n",
    "\n",
    "#### 4.1 Exploration-Exploitation Trade-off\n",
    "\n",
    "**Meta-RL에서의 불확실성 활용**:[14][15][9]\n",
    "\n",
    "**Risk-aware Meta-level Decision Making** (2022):[9]\n",
    "- 로봇 탐사에서 **epistemic uncertainty**로 탐사 전략 결정\n",
    "- 불확실성 높은 영역 → 탐사 가치 높음\n",
    "- 불확실성 낮은 영역 → 활용(exploitation) 전략[9]\n",
    "\n",
    "**Meta-reinforcement learning for quantum control** (Nature, 2025):[14]\n",
    "- 양자 시스템의 불확실성 존재 하 제어\n",
    "- 메타 학습으로 **환경 변화에 강건한 제어**\n",
    "- 내부 루프(specific task)와 외부 루프(meta-learning) 이중 구조[14]\n",
    "\n",
    "**Grasp Learning with Uncertainty** (2018):[15]\n",
    "- **Epistemic uncertainty**: 지식 부족으로 인한 불확실성 → 탐사로 감소 가능\n",
    "- **Aleatoric uncertainty**: 데이터 노이즈 → 탐사로 감소 불가\n",
    "- **Epistemic만 활용**한 탐사가 훨씬 효과적[15]\n",
    "\n",
    "#### 4.2 Uncertainty의 두 가지 유형과 의사결정\n",
    "\n",
    "**Epistemic Uncertainty (인식론적 불확실성)**:[16][15][12][7]\n",
    "- **원인**: 모델의 지식 부족, 학습 데이터 부족\n",
    "- **특징**: 추가 데이터로 감소 가능\n",
    "- **Decision making**: \n",
    "  - 높은 epistemic uncertainty → 더 많은 데이터 수집 필요[15]\n",
    "  - Active learning의 샘플 선택 기준[15]\n",
    "  - 탐사(exploration) 전략에 활용[16][9]\n",
    "\n",
    "**Aleatoric Uncertainty (우연적 불확실성)**:[15][12][7]\n",
    "- **원인**: 데이터 자체의 노이즈, 측정 오류\n",
    "- **특징**: 더 많은 데이터로도 감소 불가\n",
    "- **Decision making**:\n",
    "  - 높은 aleatoric uncertainty → 본질적으로 예측 어려운 케이스\n",
    "  - 리스크 관리: 보수적 결정 필요[7]\n",
    "  - 예측 거부(rejection) 고려[7]\n",
    "\n",
    "#### 4.3 Uncertainty-guided Meta-Learning\n",
    "\n",
    "**UAPML: Uncertainty-Aware Prompted Meta-Learning** (2024):[5]\n",
    "\n",
    "**핵심 아이디어**:[5]\n",
    "- **Bayesian meta-prompt**: 메타 프롬프트 자체에 불확실성 부여\n",
    "- 사후 불확실성이 태스크별 프롬프트 불확실성과 일치함을 이론적으로 증명[5]\n",
    "- **Hard vs Soft 방식**: 불확실성에 따라 자동으로 프롬프트 구성 방식 선택[5]\n",
    "\n",
    "**계산 효율성**:[5]\n",
    "- 모델 백본 고정, 프롬프트만 조정\n",
    "- 전체 가중치 업데이트 대비 **80% 이상 계산 비용 감소**\n",
    "- 성능 저하 없이 빠른 적응[5]\n",
    "\n",
    "### 5. HyperNetworks 논문에 Bayesian을 적용할 구체적 방법\n",
    "\n",
    "#### 5.1 정적 하이퍼네트워크의 Bayesian 확장\n",
    "\n",
    "**원논문의 접근**:[3]\n",
    "```\n",
    "z_i: fixed layer embedding (learnable parameter)\n",
    "W_i = f_θ(z_i)\n",
    "```\n",
    "\n",
    "**Bayesian 확장 방안**:\n",
    "```\n",
    "z_i ~ N(μ_z, Σ_z): 임베딩의 사후 분포\n",
    "W_i = f_θ(z_i): 가중치 생성\n",
    "p(W_i | data) = ∫ p(W_i | z_i) p(z_i | data) dz_i\n",
    "```\n",
    "\n",
    "**구현 방법**:[1][2]\n",
    "- Variational Inference로 $$q(z_i)$$ 학습\n",
    "- Reparameterization trick: $$z_i = \\mu_z + \\sigma_z \\odot \\epsilon$$, where $$\\epsilon \\sim N(0,1)$$\n",
    "- 여러 샘플로 앙상블 예측[2]\n",
    "\n",
    "#### 5.2 동적 하이퍼네트워크의 Bayesian 확장\n",
    "\n",
    "**원논문의 동적 임베딩**:[3]\n",
    "- LSTM의 hidden state가 시간에 따라 변하며 가중치 생성\n",
    "- 입력 시퀀스에 적응적\n",
    "\n",
    "**Bayesian 해석**:[2][4]\n",
    "```\n",
    "h_t: LSTM hidden state at time t\n",
    "z_t = g(h_t): dynamic embedding\n",
    "(μ_W(t), σ_W(t)) = Hypernetwork(z_t)\n",
    "W_t ~ N(μ_W(t), diag(σ_W(t)^2))\n",
    "```\n",
    "\n",
    "**Uncertainty 전파**:\n",
    "- 시간에 따른 epistemic uncertainty 변화 추적\n",
    "- 초반에는 높은 불확실성, 더 많은 정보 관찰 시 감소\n",
    "- **Decision making**: 불확실성 임계값 이하일 때만 예측 출력[12][7]\n",
    "\n",
    "#### 5.3 Layer Normalization과 Bayesian의 결합\n",
    "\n",
    "논문에서 **Layer Norm + HyperLSTM**이 효과적임을 보였습니다.[3]\n",
    "\n",
    "**Bayesian 관점의 해석**:[17]\n",
    "- Layer Normalization은 **암묵적 정규화** 효과\n",
    "- Bayesian 프레임워크와 결합 시 사후 분포의 분산 안정화[17]\n",
    "- 불확실성 추정의 품질 향상[17]\n",
    "\n",
    "### 6. 실용적 구현 전략\n",
    "\n",
    "#### 6.1 경량 Bayesian HyperNetworks\n",
    "\n",
    "**문제**: Full Bayesian inference는 계산 비용이 높음[2][4]\n",
    "\n",
    "**해결책**:[2][4]\n",
    "1. **Universal weights는 point estimate**: 공통 지식은 deterministic\n",
    "2. **Task-specific weights만 Bayesian**: 불확실성이 중요한 부분만\n",
    "3. **Low-rank approximation**: $$\\Sigma = LL^T$$로 분산 행렬 저차원화[2]\n",
    "\n",
    "#### 6.2 Ensemble 기반 접근\n",
    "\n",
    "**Deep Ensemble과 HyperNetworks 결합**:[18][15]\n",
    "\n",
    "```python\n",
    "# Multiple hypernetworks\n",
    "hypernetworks = [HyperNet() for _ in range(N)]\n",
    "\n",
    "# Generate ensemble predictions\n",
    "predictions = []\n",
    "for hyper in hypernetworks:\n",
    "    W = hyper.generate_weights(task_embedding)\n",
    "    pred = main_network(input, W)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Uncertainty estimation\n",
    "mean_pred = mean(predictions)\n",
    "epistemic_unc = variance(predictions)\n",
    "```\n",
    "\n",
    "**장점**:[18]\n",
    "- 구현 간단\n",
    "- 병렬화 용이\n",
    "- Gradient 기반 학습 가능\n",
    "\n",
    "#### 6.3 MC Dropout with HyperNetworks\n",
    "\n",
    "**Variational inference의 근사**:[17]\n",
    "\n",
    "```python\n",
    "# Enable dropout during inference\n",
    "hypernetwork.train()  # Keep dropout active\n",
    "\n",
    "predictions = []\n",
    "for _ in range(K):  # K samples\n",
    "    W = hypernetwork(task_embedding)\n",
    "    pred = main_network(input, W)\n",
    "    predictions.append(pred)\n",
    "\n",
    "uncertainty = std(predictions)\n",
    "```\n",
    "\n",
    "**계산 효율성**:[17]\n",
    "- 단일 모델만 필요\n",
    "- Inference 시 multiple forward pass\n",
    "- 추가 학습 비용 없음\n",
    "\n",
    "### 7. 향후 연구 방향과 제안\n",
    "\n",
    "#### 7.1 HyperNetworks + Bayesian + Meta-Learning 통합\n",
    "\n",
    "**제안하는 프레임워크**:\n",
    "\n",
    "1. **Outer loop (Meta-learning)**:\n",
    "   - 다양한 태스크에서 하이퍼네트워크 파라미터 학습\n",
    "   - 태스크 분포 $$p(T)$$에서 샘플링[19][2]\n",
    "\n",
    "2. **Middle loop (HyperNetwork)**:\n",
    "   - 태스크 임베딩에서 가중치 분포 생성[1][2]\n",
    "   - $$(μ_W, σ_W) = H_φ(z_{\\text{task}})$$\n",
    "\n",
    "3. **Inner loop (Bayesian Inference)**:\n",
    "   - 소수의 샘플로 빠른 적응[1][2]\n",
    "   - Uncertainty 정량화로 예측 신뢰도 제공[12][7]\n",
    "\n",
    "#### 7.2 Decision Making 응용 시나리오\n",
    "\n",
    "**금융 의사결정**:\n",
    "- 시장 변화 → 새로운 태스크\n",
    "- HyperNetwork로 빠르게 새 전략 생성\n",
    "- Uncertainty로 리스크 관리\n",
    "- 불확실성 높은 시기 → 보수적 포트폴리오\n",
    "\n",
    "**자율주행**:\n",
    "- 다양한 날씨/도로 조건 → 태스크\n",
    "- Meta-learning으로 새 조건 빠른 적응\n",
    "- Epistemic uncertainty로 탐사 영역 결정\n",
    "- 높은 불확실성 → 인간에게 제어 이양[9]\n",
    "\n",
    "**의료 진단**:[4]\n",
    "- 환자 특성 → 태스크\n",
    "- Few-shot으로 희귀 질환 진단\n",
    "- Uncertainty로 추가 검사 필요성 판단\n",
    "- Aleatoric uncertainty 높음 → 예측 불가능, 경과 관찰\n",
    "\n",
    "### 결론\n",
    "\n",
    "HyperNetworks 논문은 **Bayesian 프레임워크와 결합할 수 있는 풍부한 요소**들을 가지고 있습니다:\n",
    "\n",
    "**주요 결합 요소**:\n",
    "- 가중치 생성 메커니즘 → 확률 분포로 확장\n",
    "- 임베딩 벡터 → 불확실성 부여\n",
    "- 동적 가중치 → 시간에 따른 불확실성 변화\n",
    "\n",
    "**최신 연구 현황**:\n",
    "- BayesianHMAML (2022): Few-shot learning + uncertainty[1][2]\n",
    "- Meta-Mol (2025): 약물 발견 + Bayesian hypernetwork[4]\n",
    "- UBMF (2025): 결함 진단 + uncertainty-aware meta-learning[7]\n",
    "- SurvUnc (2024): 생존 분석 + meta-model uncertainty[12]\n",
    "\n",
    "**Decision Making에의 기여**:\n",
    "- **Epistemic uncertainty**: 탐사 전략, active learning[16][9][15]\n",
    "- **Aleatoric uncertainty**: 리스크 관리, 예측 거부[7]\n",
    "- **Calibrated uncertainty**: 신뢰할 수 있는 의사결정[11][10]\n",
    "\n",
    "\n",
    "[1](https://arxiv.org/abs/2210.02796)\n",
    "[2](https://openreview.net/pdf?id=Z4Kexjh34vT)\n",
    "[3](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/39513708/02a09e34-d8a8-4a28-a8ee-7238d4313d55/HyperNetworks.pdf)\n",
    "[4](https://pmc.ncbi.nlm.nih.gov/articles/PMC12354953/)\n",
    "[5](https://openreview.net/forum?id=rDuqo9KTzh)\n",
    "[6](https://openreview.net/forum?id=mQ72XRfYRZ)\n",
    "[7](https://www.sciencedirect.com/science/article/abs/pii/S0950705125008184)\n",
    "[8](https://github.com/gmum/few-shot-hypernets-public)\n",
    "[9](https://arxiv.org/abs/2209.05580)\n",
    "[10](https://ieeexplore.ieee.org/document/10885944/)\n",
    "[11](https://www.themoonlight.io/ko/review/bayesian-meta-learning-for-trustworthy-uncertainty-quantification)\n",
    "[12](https://arxiv.org/html/2505.14803v1)\n",
    "[13](https://dl.acm.org/doi/10.1145/3711896.3737140)\n",
    "[14](https://www.nature.com/articles/s41534-025-01034-9)\n",
    "[15](https://arxiv.org/html/2309.12038v2)\n",
    "[16](https://arxiv.org/html/2403.06826v1)\n",
    "[17](https://ieeexplore.ieee.org/document/10305157/)\n",
    "[18](https://www.nature.com/articles/s41524-025-01572-y)\n",
    "[19](https://meta-learn.github.io/2020/papers/38_paper.pdf)\n",
    "[20](https://www.themoonlight.io/ko/review/generalization-bounds-via-meta-learned-model-representations-pac-bayes-and-sample-compression-hypernetworks)\n",
    "[21](https://proceedings.mlr.press/v180/song22a/song22a.pdf)\n",
    "[22](https://www.research-collection.ethz.ch/bitstreams/b5b21caf-40ad-47c0-9c0b-bde269e942f6/download)\n",
    "[23](https://www.sciencedirect.com/science/article/pii/S240589632500093X)\n",
    "[24](https://proceedings.neurips.cc/paper/2020/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf)\n",
    "[25](https://arxiv.org/abs/2410.13577)\n",
    "[26](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610392.pdf)\n",
    "[27](https://arxiv.org/abs/2405.19931)\n",
    "[28](https://dl.acm.org/doi/10.1145/3709026.3709116)\n",
    "[29](https://proceedings.mlr.press/v230/okanik24a.html)\n",
    "[30](https://arxiv.org/abs/2210.11737)\n",
    "[31](https://dl.acm.org/doi/10.1007/s10489-024-06127-0)\n",
    "[32](https://repository.rit.edu/context/theses/article/13173/viewcontent/DPandeyDissertation1_2025.pdf)\n",
    "[33](https://www.sciencedirect.com/science/article/abs/pii/S0266892025001262)\n",
    "[34](https://publikationen.bibliothek.kit.edu/1000173441/154117629)\n",
    "[35](https://seunghan96.github.io/assets/pdf/BNN/paper/30.Uncertainty%20quantification%20using%20Bayesian%20neural%20networks%20in%20classification_Application%20to%20ischemic%20stroke%20lesion%20segmentation%20(2018).pdf)\n",
    "[36](https://proceedings.neurips.cc/paper_files/paper/2023/file/2a4310c4fd24bd336aa2f64f93cb5d39-Paper-Conference.pdf)\n",
    "[37](https://www.nature.com/articles/s41598-025-24093-6)\n",
    "[38](https://proceedings.mlr.press/v162/xie22c/xie22c.pdf)\n",
    "[39](https://www.sciencedirect.com/science/article/abs/pii/S0925231221019068)\n",
    "[40](https://indico.cern.ch/event/1208723/contributions/5230073/attachments/2600859/4521507/An%20Introduction%20to%20Bayesian%20Neural%20Network%20and%20Uncertainty%20Quantification%20in%20Deep%20Learning-Jacopo%20Talpini.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
